The use of shallow methods for the machine comprehension of text has been well explored in previous work. Using a bag of words algorithm for textual entailment tasks was pioneered by \cite{deepread}, this matches question-answers pairs to sentences in the text and chooses the best pair with the best matching sentence. This algorithm counts the words in common between the pair and the sentence and uses that as the metric to decide on the answer. These systems ignore predicate-argument structure and can easily fail in the presence of quantifiers, negations or synonyms. Furthermore in \newcite{behaviour_best} such methods are classified as being ``cheap tricks" that are deceptive, as they do not prove the true comprehension of text.

Other work combines the bag of words algorithm with more complex means of relating questions to the answers by representing each pair as vectors and them evaluating them in a shared vector space \cite{deep_selection}. In this case the bag of words is used to construct the vector representation of a sentence. The result is a vector with the number of mentions for each individual word in that sentence, which is then normalized by the length of the sentence. In the same paper they show that using a bigram model created with a convolutional neural network is superior to the bag of words model.

The bag of words model can also be used to generate a feature space for a scoring function \cite{memory_networks}. This can be made to create multidimensional feature spaces by having each dimension represent a different sentence that we want to examine. Such a function can be used to train a neural network to match facts with sentences and produce an answer for a question. In the case of an unseen word then the system would also use bag of words to store the context in which that word was used, generating a left and right bag.  

Apart from the above bag of words approaches, other work focuses on the use of feature engineering. For example, \newcite{learning_entailment} designs a set of 28 features and then train a classifier to determine textual entailment. Examples of the features included are: polarity, antonymy, modality, quantifiers, numerical and temporal, etc. Textual alignment can be used to automatically extract features as demonstrated in \cite{relation_alignment}, which has the advantage of being adaptive as opposed to having static features. 

More complex methods which strive to derive deeper knowledge than the shallow methods used in our system make use of Natural Logic as in \cite{mccartney2007manning}. This decomposes the problem into a set of small local edits, which transform the hypothesis into the premise. In practice, the hypothesis is the question answer pair and the premise is a sentence from the text the question refers to. \newcite{angeli2014naturalli} combines Natural Logic with a distributional model to obtain a likely valid derivation (with a confidence interval) and to compute for multiple premises simultaneously. 

Our work explores the limits of shallow methods and also how they compare with each other. As opposed to most work focusing only on one method and refining it, we use a broad spectrum of different improvements to a basic bag of words baseline as well as build our own heuristic system. 