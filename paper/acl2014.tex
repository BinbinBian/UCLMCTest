%
% File acl2014.tex
%

\documentclass[11pt]{article}
\usepackage{framed}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{listings}

\def\subtitle#1\par{\removelastskip\bigskip\hrule
   \noindent\vrule height12pt depth5pt width0pt #1\par
   \hrule\nobreak\medskip
}
\def\begalg{\removelastskip\medskip
  \bgroup \parindent=0pt \tt \obeylines \obeyspaces  
          \everymath{\catcode`\ =10 \catcode`\^^M=5 }
}
\def\endalg{\egroup\medskip} 
{\obeyspaces\global\def {\ }}

\title{Solving MCTest with semantic textual similarity and matching rules??}

\author
       {A. Vlachos, T. Brown, N. Greco, G. Mocanu and E. Smith
       \\
       Computer Science Department\\
	University College London\\
       \tt{\{a.vlachos,t.brown,n.greco,g.mocanu,e.smith\}@cs.ucl.ac.uk}\\ 
       }

\date{}

\begin{document}
\maketitle
\begin{abstract}
MCTest is a recently developed test for evaluating Machine Comprehension.
Our approach to the task is through textual similarity and shallow methods.
We build a simple bag-of-words baseline and we enhance it through additional pre-processing (i.e. coreference resolution, hypernym).
We build a set of features and train a logistic classifier that we use to score multiple-choice answers.
Finally we show how the introduction of simple matching rules system outperform current results on the MCTest.
\end{abstract}

\section{Introduction}
% Emil ? % Tom X Ellery ?
%1 page

\section{Previous work}
% Nicola X % Tom ? % Emil ? Ellery ?
%1 page

\newcite{deep_selection} uses bag-of-words as a baseline combining this shallow method with convolutional neural networks to capture complex semantics of a sentence.

\newcite{deepread} proposed D{\small EEP}R{\small EAD} with shallow methods for story comprehension and combine them with some heuristics to answer questions `who / what / when".

\section{Task description}
% Tom ? % Emil X Ellery ?
%1/2 a page--

The task has been presented in  \cite{mctest}

\section{Baseline}
% Nicola X % Ellery ?
We propose a baseline system, a simpler version than the originally proposed by \newcite{mctest} only using simple lexical feature.
This system matches a bag of words constructed from the question and a candidate answer with each sentence in the story.
The question-answer pair with the most words in common is considered the best candidate answer.
Normalization are applied such as removing stop words stemming to remove affixes from words.

This semantic overlap approach treats the problem as a textual similarity task and would perform best when the pair is a subset of a sentence.
However, this strategy ignores predicate-argument structure and can easily fail in the presence of quantifiers, negations or synonyms.
Work on story comprehension using bag-of-words has a long history, \newcite{deepread} proposed D{\small EEP}R{\small EAD} and showed how such systems with some heuristics can achieve high accuracy especially on questions with ``who / what / when", which is most part of our questions.

We will build upon this baseline in Section~\ref{sec:bagofwords} and combine it with heuristics in Section~\ref{sec:rulebased}.
Results for MC160 and MC500 are shown in Table \ref{tab:resultBOWMC160} and Table \ref{tab:resultBOWMC500}.
The baseline has been authored without seeing both the test sets.

\begin{figure}

\subtitle Definitions

Passage $P$, $P_i$ sentence $i$ in passage $P$, set of words in question Q, set of words in candidate answer $A_{1..4}$ and set of stop words $X$.

\subtitle Algorithm 1 Sentence level bag-of-words

\begalg
for $i$=1 to 4 do
   $S$ = $A_i \cup Q$
end for
return $sw_{1\ldots4}$
\endalg

\caption{\label{fig:mct_example} Lexical-based baseline algorithm }
\end{figure}

\section{Preprocessing}
% 2 pages
% an alternative version -Ellery

Matching question-answer pairs with the story can be significantly improved by homogenising the format of all stories and question-answer strings. Our matching algorithms operate on raw textual tokens, which are lemmatised and stripped of all extraneous function words; however, the raw format was generated on-demand, rather than during the pre-processing stage, and we retained the deep grammatical structure of the text in order to dynamically alter the format based on certain question conditions.
%transform the two to become more {\em similar} to each other and removing words that are noise in the text.
%Previously discussed pre-processing of the text was removing stop words, tokenizing the text into sentences and into words, stemming words, and adding POS notation for which we used the Stanford Parser\footnote{Some note on the Stanford parser}
%TODO

We focused on four pre-processing stages that will be discussed in this section: % Is the sentence selection really pre-processing? -ellery
syntactic parsing and coreference resolution, hypernym annotation, sentence selection and combining question and answer.

\subsection{Syntactic Analysis}
% Emil X % Ellery X

The initial pre-processing stage used the Stanford Parser REFERENCE and Stanford Dependencies REFERENCE to obtain phrase-structure and dependency trees for each story, along with its questions and answers. We also made use of the lemmatization and part-of-speech tagging provided by this system. This toolkit performed well on the given data, due to the intentional linguistic simplicity of the stories. Of the few inconsistencies, the majority were due to incorrect recognition of invented brand names (e.g. “Cookies n’ Crème” and “Friendly-O’s”) and the inability to categorize some subordinate dependencies. However, such cases were rare, and the errors introduced at this pre-processing stage were negligible in the final results.

Following this, coreference information was extracted using the Stanford CoreNLP package REFERENCE. The passage text was parsed independently of the question and answer strings, so all coreference chains were local to the story itself. Resolving links between a question and its answer strings proved to be detrimental to performance. We used an out-of-the-box configuration of the coreference rules, as this was deemed to perform adequately on the simplisitic format of the given stories. Some errors emerged when resolving coreferences involving multiple entities, however, but correcting these errors is beyond the scope of this work.

\subsection{Hypernym}
% Ellery X
\subsection{Sentence selection}
% Nicola X
The current baseline chooses the best candidate on the basis of how much it matches one sentence in the corpus.
This is a clear disadvantage give that the MCTest has questions whose answer may be in contained in multiple sentences;
at the same time, running matching the bag of words between the text entire text with the question-answer pair would give poor results  since (%TODO give a better reason
).

To improve on answering questions using multiple sentences, we propose to run the bag-of-words on the $n$ most relevant sentences for answering the question.
This problem reduces to an information retrieval task.
The idea of retrieve the sentences with the highest relevance for question answering has been already proved successful in the literature \cite{qa_techniques, deep_selection}.
In order to rank the sentences in the story we will have a scoring function $selectScore(query, document)$, that given a $query$ and a $document$, it will %TODO

We will need a scoring function that will score each sentence
%question

 For the purpose of this paper we are going to re-use our bag-of-words combined with hypernym and coreference to rank the most relevant sentences (in details in Section~\ref{sec:bigmix})

%Question


(Some is q, some is qa)  By tuning on both the {\small MC160} and {\small MC500} training set, $n=3$ gets the highest results.

Reduces the task to an information retrieval
how to score the best answers

\subsection{Question answer combiner}
% Tom X

\section{Strategy}
%1.5 pages
\subsection{Bag of words}
\label{sec:bagofwords}
% Nicola X % Ellery ?
several features
BOWNN
BOW Complement
Bow ALL

The great 6

\subsection{Scoring function}
% Nicola X
equations of deep selection

scoring question answer pairs
the choice of the SVM and gridsearch
\label{sec:bigmix}

\subsection{Rule based system}
\label{sec:rulebased}
% Ellery X

\section{Experiments and results}
% Nicola ? % Emil ? % Tom?
%1 page

\section{Evaluation of strategies}
% Ellery X
%3 pages

\section{Future work and conclusion}
% Ellery ? Tom ? Emil?
%1/2 a page

Semantic overlap is typically a symmetric relation while textual entailment is clearly not, this is a serious limitation of our baseline and the systems built on top.
However, the great results show how really simple methods can achieve great results on the MCTest.

Wordnet for synonym as well in addition to hypernym

\section*{Acknowledgments}
Thanks to

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{ref}
\end{document}
