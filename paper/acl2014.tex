%
% File acl2014.tex
%

\documentclass[11pt]{article}
\input{settings.tex}
\title{Exploring the limits of shallow approaches on the MCTest}
% Building upon a lexical system for solving the MCTest ?? or something... ~ellery
\author
       {T. Brown, N. Greco, G. Mocanu and E. Smith
       \\
       Department of Computer Science\\
	University College London\\
       \tt{\{t.brown,n.greco,g.mocanu,e.smith\}@cs.ucl.ac.uk}\\ 
       }
\date{}

\begin{document}
\maketitle

\begin{abstract}
\input{./sections/Abstract.tex}
\end{abstract}

\begin{figure}[!th]
\input{./figures/exampleMCT.tex}
\caption{\label{fig:exampleMCT} Story Q0 extracted from the MC160 development set}
\end{figure}

\section{Introduction}
\input{./sections/Introduction.tex}

\section{Previous work}
\input{./sections/PreviousWork.tex}

\section{Task description}
\input{./sections/TaskDescription.tex}

\section{Scoring function}
\input{./sections/ScoringFunctionIntro.tex}

    \subsection{Token-matching baseline}
    \input{./sections/TokenMatchingBaseline.tex}

    \subsection{Sentence selection}
    \input{./sections/SentenceSelection.tex}

    %\subsection{Pre-processing}
    % \input{./sections/PreprocessingIntro.tex}

    \begin{figure*}[!th]
    \input{./figures/preprocessing.tex}
    \caption{\label{fig:preprocessing}Examples of different transformations: (1) The initial sentence with co-reference annotation, (2) Each token is lemmatised and stop words are marked in red, (3) Hypernym for each token associated with their score }
    \end{figure*}

        % TODO
        % \subsubsection{Syntactic Analysis}
        % \input{./sections/SyntacticAnalysis.tex}

        \subsection{Co-reference resolution}
        \input{./sections/Coreference.tex}

        \begin{table}[!th]
        \centering
        \input{./figures/resultBOWcoref.tex}
        \caption{\label{table:resultBOWcoref}BOW with coref}
        \end{table}

        \subsection{Hypernymy and Synonymy}
        \input{./sections/HypernymyAndSynonymy.tex}

        % \subsubsection{Question answer combiner}
        % \input{./sections/QuestionAnswerCombiner.tex}


\subsection{Combining shallow methods}
\label{sec:combined}

We improved the simple token-matching baseline combining it with pre-processing techniques and sentence selection. Each of the baseline achieves different results. 

\begin{itemize}
\item \textbf{Baseline+COREF} \\
We
\item \textbf{Hyperbow}
\item \textbf{BOWALL+COREF with sentence selection using BOWALL+COREF on q}
\item \textbf{BOWALL+COREF with sentence selection using BOWALL+COREF on qa}
\item \textbf{HYPERBOW with sentence selection with BOWALL+COREF on qa}
\item \textbf{BOWALL with  with sentence selection using HYPERBOW on QA}
\end{itemize}

After an analysis of the results in the test set, we found that some baseline predict the same score to multiple answers, hence combining the different methods we can gain %TODO something

\begin{table}[!th]
\input{./figures/resultBOWs.tex}
\caption{\label{table:results}Percent correct on MC160 and MC500 dev+train sets.}
\end{table}


In order to build such classifier we built a set of features that combine bag of words with sentence selection, hypernymy, co-reference, we trained a linear SVM to estimate the class of each sentence. Given that we model we described, in order to associate a score to the binary classification, we calibrate the probability of being part of a class, using Platt scaling \cite{plath_scaling} - logistic regression applied on SVM's score, these are fit by a further cross-validation on the training data. Finally we do parameter optimization through grid search.

\section{Rule-based system}
\label{sec:rulebased}
% Ellery X

In addition to the previous methods, we also developed a rule-based scoring system. This applies a series of transformations to the story text, and assigns weights to each word, based on the conditions required to answer a given question. For example, questions requiring negation will cause the weight value for each word to be inverted, since we will need to search for the answer that is least likely to be inferred by the story.

Each rule consists of a syntactic or lexical pattern, which filters question-answer pairs into categories, according to certain empirically-designed criteria. If a question falls into one or more of these categories, a corresponding story transformation procedure is invoked. Each procedure may add or remove words from the story, or alter the weight value associated with each word. There are several base transformations applied for all questions, including lemmatization and removal of stopwords.

Some examples of our rule-based question filters are:

\textit{Negation}. Negation covers questions such as ``What did James not eat for dinner?''. These questions are a distinct special-case, which cannot be solved by simply selecting the answer with the highest similarity to the story, as all of our other methods do. We detected negative questions using the syntactic dependency graph of the question string: a question is negative if and only if the sentential root is linked to any other node by a negated dependency.

\textit{Character-subject}. Character-subject questions are those which directly concern the actions of a named character, such as ``Why did Jon go to the park?''. Any question with a named entity as a nominal-subject dependent of the question's head verb was deemed to be a character-subject question.

\textit{Narrative}. Narrative questions are those which pertain to the structure of the story itself, for example ``Which character was first mentioned in the story?''. These questions necessitate that a given system understands the concept of a story, which will have characters and a defined narrative flow, rather than simply viewing it as an unstructured piece of text. We detected these instances using a series of cue-words, such as ``story'', ``passage'' or ``character''. These cues indicate that the question acknowledges the concept of a story.

\textit{Temporal}.

\textit{Implicative}.



% In addition to the previous methods, we also developed a rule-based scoring system. This applies a series of transformations to the story text, and assigns weights to each word, based on the conditions required to answer a given question. Each rule consists of a syntactic or lexical pattern, which filters question-answer pairs according to certain empirically designed criteria. Examples of such filters include negated questions, ``why'' questions, or questions requiring temporal recognition of events.

% If a question falls into one or more of these categories, a corresponding story transformation procedure is invoked. Each procedure may add or remove words from the raw textual output of the story, or alter the weight value associated with each word. There are several base transformations applied to all questions, including lemmatization, removal of stopwords, and inverse term frequency weighting.

% After these transformations have been applied for a given question-answer pair, the weighted tokens are then passed to a scoring function, such as the weighted bag-of-words used in Algorithm 2. We found that the best performance, however, came from utilising a modified version of the Sliding Window with Distance algorithm used in \cite{mctest} as our base scoring function. In combination with the rule-based system and SVM, we managed to exceed the performance of the algorithm in its original form. Additionally, in the case of the MC160, we were able to outperform the author's implementation of the same algorithm combined with a Recognising Textual Entailment system.


% While we registered an improved result, it should be noted that the improvements of the rule-based transformations were less significant when compared to the effect of the simpler transformations, such as lemmatization. However, it was not our intent to over-engineer these rules when it would be built upon a relatively shallow lexical baseline. As such, all transformations were somewhat superficial heuristic techniques. The primary goal of this implementation was to categorize and analyze the performance of a given scoring system, and to gain an insight into the flaws of both the system and the test itself, which are presented in the Evaluation section. Additionally, it does demonstrate a proof-of-concept for a deeper rule-based system, with more sophisticated transformation algorithms on top of a stronger baseline.


%
%
%\begin{figure}
%
%\subtitle Definitions
%
%Passage $P$, $W(w)$ weight of word $w$, question $Q$, answers $A_{1..4}$ and a set of rule-transformation pairs $(R,T)$.
%
%\subtitle Algorithm 2: Question filtering
%
%\begalg
%for each $(R,T)$ do
%  if $R(Q,A)$ holds then
%    $P = T(P)$
%end for\\
%for $i = 1\ldots4$ do
%  $S = (A_i \cup Q) \cap P$
%%$r_i = sc(P,S)$
%  $\displaystyle r_i = \sum_{j=0}^{\left |  S\right |} W_{S_j}$
%end for\\
%return $r_{1\ldots4}$
%\endalg
%
%\caption{\label{fig:rulebasedalg}Rule-based filtering algorithm }
%\end{figure}


\section{Experiments and results}

Our baseline under performs the MCTest baseline.

We combined the six BOW-based methods and we classify it with our SVM scoring function.

The rule-based system (RBS) outperforms the on the MC160 Test set, hence the SVM combined with the RBS. The SVM improves of about 5\% the RBS on the MC500, beating the MCTest baseline, but still underperforming compared to the RTE results.

\subsection{Combining RBS with shallow methods}

\subsection{Combining RBS with RTE}
To make our results comparable with the second baseline in \newcite{mctest}, we augmented our rule-based system with the state of the art RTE system BIUTEE \cite{biutee} with default settings. Even in this case, our combination outperforms their result 

\begin{table}[!th]
\begin{tabular}{|l|c|c|}
\hline
 & MC160 & MC500 \\ \hline
Baseline         & 54.90\%  & 50.72\%  \\ \hline
% TM+Co-reference   & 61.81\%  & 53.24\%  \\ \hline
% TM+Hypernym   & 58.75\%  & 51.82\%  \\ \hline
% TM+Coref (selection Q)   & 60.17\%  & 48.22\%  \\ \hline
% TM+Coref (selection QA)   & 57.92\%  & 42.60\%  \\ \hline
% TM+Hypernym (selection QA).   & 59.13\%  & 51.65\%  \\ \hline
% TM+Coref (s. hypernym QA).   & 56.46\%  & 42.88\%  \\ \hline
SVM  & 67.57\%  & 56.31\%  \\ \hline
\hline
RBS   & 71.77\%  & 58.67\%  \\ \hline
RBS + SVM   & 71.35\%  & 60.17\%  \\ \hline
RBS + RTE   & \textbf{73.50}\%   & \textbf{64.20}\% \\ \hline
\hline
MCTest (SW+D)   & 68.02\%  & 59.93\%  \\ \hline
MCTest (SW+D)+ RTE   & 69.27\%  & 63.33\%  \\ \hline
\end{tabular}
\caption{Percent correct for the multiple choice questions on MC160 and MC500 test sets}
\label{table:results}
\end{table}
% Nicola ? % Emil ? % Tom?
%1 page


%TODO this is to explain sentence selection
combined with hypernym and co-reference to rank the most relevant sentences (in details in Section~\ref{sec:bigmix}). By tuning on both the MC160 and MC500 training sets, we set $n=3$ as it performs best.

\section{Evaluation of strategies}
% Ellery X

Following our experiments on the test set, we then conducted an evaluation of our scoring functions and also used the rule-based system to analyse both the MCTest dataset and the impact on performance of individual components of our rule-based system.

\subsection{bow, svm, etc. evaluation}

% maybe this should go in conclusion?
In addition, although  scoring function built on the SVM achieves good %find another word
results, its five-fold cross-validation is computationally very expensive.

\subsection{Rule-based System}

Using our rule-based system, we were able to perform a quantitative analysis on the performance improvement or degradation for each individual component. In addition, we could view the state of the story text after each transformation had been applied, giving us an insight into the effects of each filter on a per-question basis. From this, we gained a greater understanding of the differences between the MC160 and the MC500 tests, and the performance of our optimal system on each individual question category.

\begin{table}[!th]
\input{./figures/ruleres.tex}
\caption{Performance of base transformations on combined Development and Training sets. Optimal Combinations are in bold.}
\label{table:ruleres}
\end{table}

In Table~\ref{table:ruleres}, we present the accuracy scores for the Development and Training datasets, using the Sliding Window with Distance algorithm as a scoring function, along with the relative improvement of each base transformation. The base transformation routines are performed on all stories, regardless of the content of the question-answer pair. We determined the optimal combination of base transformations through observation, and this result is also displayed in Table~\ref{table:ruleres}. It is noteworthy that the optimal configuration differs from the MC160 to the MC500, in particular the absence of coreference resolution in the final MC160 result.

As would be expected, lemmatization provides a notable increase in performance, particularly in the MC500. The loss of information from matching only word stems is negated by homogenizing the format of the question, answer and story, and this relatively cheap improvement was used in all algorithms we have presented. A further inexpensive optimisation was the removal of stopwords, using a list obtained from the Natural Language Toolkit \cite{bird2006nltk}, which increased performance across all datasets. However, while we removed such words from the raw tokenized output of the story, and thus these words were not scored, they were still counted as part of the sliding window size and word distance in the scoring function. Conversely, any hypernyms added to the text were not considered when counting the distance between words.

For the hypernym expansion procedure, each grammatical token was mapped to an extended set of raw lexical tokens, based on the parent token's WordNet hypernym tree. Each additional token is weighted based on its depth in the given hypernym tree, such that less relevant relations have a smaller impact on scoring. While this did register an improvement over the baseline system, the additional words had very little impact, or were occasionally detrimental to performance, when combined with higher-impact methods, such as co-reference and token frequency. Most hypernyms, especially those further down a token's tree, were merely noise. In addition, this procedure is computationally expensive, and the low number of hypernym matches overall did not justify its inclusion in the optimal configuration.

Most notably, co-reference resolution decreases accuracy on the MC160 datasets, while having the opposite effect on the MC500. This may be due to the increased simplicity of the questions in the MC160 stories: in many cases, the correct answer string is taken verbatim from the story text, and as such, there is enough information in the unaltered story to identify the relevant sentences. For these questions, which are much more prevalent in the MC160 than in the MC500, resolving co-references will give a higher score to the incorrect answers, which introduces inaccuracies into an otherwise clear match. In addition, the mistakes inherent to even a state-of-the-art co-reference resolution system may provide misleading information. An example from the MC160 Test set would be ``Todd is a small boy from the town of Rocksville'' becoming ``Todd is Todd'' or ``Todd is Todd a small boy from the town of Rocksville''. In either case, a simpler system would suffice. However, the MC500 questions are more subtly designed, and benefit from the additional information provided by co-reference resolution.

Multiplying the weight of each word by the inverse logarithm of its frequency in a given story produces the most significant improvement of all base transformations, and is computationally insignificant when these frequencies are calculated during the pre-processing stage. This is a common technique used in Information Retrieval systems, though the document size is typically much larger than in this task, and requires no syntactic or semantic information. Interestingly, this lexical trick has a greater effect on accuracy than much deeper methods such as coreference.

\subsection{Question Analysis}
% ellery X

Using the question-filtering rules, we were able to obtain an individual accuracy score for each question category on our optimal baseline, and a comparison with the overall score is presented in Table \ref{table:catres}. Note that while we improved upon many of these scores in our final rule-based system, here we present the baseline scores in order to analyze the performance of a ``pure'' bag-of-words system.

\begin{table}[!th]
\centering
\begin{tabular}{c|c|c|}
\cline{2-3}
                                                 & \multicolumn{2}{|c|}{\textbf{Relative Accuracy}} \\ \hline
\multicolumn{1}{|c|}{\textbf{Category}} & \textbf{MC160}                 & \textbf{MC500}                \\ \hline
\multicolumn{1}{|c|}{\textit{Negation}}          & \textbf{-55.03\%}          & \textbf{-28.98\%}         \\ \hline
\multicolumn{1}{|c|}{\textit{"Why" Questions}}   & +1.26\%                    & +6.23\%                   \\ \hline
\multicolumn{1}{|c|}{\textit{Character Subject}}   & \textbf{-5.71\%}           & -1.65\%                   \\ \hline
\multicolumn{1}{|c|}{\textit{Narrative}}         & \textbf{-7.83\%}           & \textbf{-20.59\%}         \\ \hline
\multicolumn{1}{|c|}{\textit{Temporal}}          & -1.39\%                    & +5.69\%                   \\ \hline
\multicolumn{1}{|c|}{\textit{Implicative}}       & -3.66\%                    & +4.59\%                   \\ \hline
\end{tabular}
\caption{Performance of the optimal baseline on a selection of question categories, in comparison to the overall score. Notable difficult cases are highlighted.}
\label{table:catres}
\end{table}

There are several clear deficiencies in certain areas, particularly in handling negation. These errors provide a broad overview of the cases in which simple lexical techniques are not sufficient to determine the correct answer.

We detected negative questions using the syntactic dependency graph of the question string. A question is negative if and only if the sentential root is linked to any node by a negated dependency. We identified two primary types of negative questions: direct negation and indirect negation. Questions with direct negation are of the form ``What wasn't in the story?'', for example, and it is sufficient to locate the singular answer which is not referenced in the passage. Indirect negation encompasses all other questions with a negated headword.

Instances of direct negation are impossible to answer using a standard word-matching algorithm, without special provision for such cases. The MC160 contains a larger proportion of these questions, and thus our baseline registers better performance on the MC500, with fewer such cases.

However, these instances of simple negation are trivial to solve, and our final system implements a rule for such questions, and answers all correctly. However, the remaining negative questions, which are a significant majority in the MC500, have proven to be more difficult to engineer. We attempted a shallow heuristic transformation using the explicit negation cues from \newcite{councill2010s}, which registered minor improvements.

Additionally, we observed that, for questions which directly relate to the actions of a named entity, it is likely that the relevant instance will be referred to in pronominal form in the story text. To study these cases, we utilised syntactic dependencies, combined with part-of-speech tagging, to detect the presence of a proper noun subject for each question. It can be seen that, as consequence of removing co-reference from the MC160 baseline, questions directly concerning a named character under-perform, in comparison to the MC500. However, this is a necessary consequence in order to increase the overall performance on the MC160.

The performance of our system on more abstract questions, concerning the overall narrative of the story, also demonstrates a significant inadequacy of lexical-based algorithms. Questions such as ``What was the first character mentioned in the story?'', which relate to the overall narrative flow of the passage, and general questions concerning the state of the story environment, such as ``Where is the story set?'', are difficult to solve without a system which understands the concept of a story itself. Traditional question-answering methods would also struggle here, and it seems as though a model designed specifically for reading comprehension contexts is required to truly perform well at this task.

\subsection{MC160 and MC500 Comparison}
%ellery X

It is evident both from our results, and the original baseline, that the MC500 corpus is significantly more challenging than the MC160. However, this may be primarily due to the inferior design of the MC160, which was created prior to the MC500. Our observations show that the MC160 can be more easily beaten by so-called ``cheap tricks'', while the MC500 has proved more resilient to the same optimisations. The base perfomance of our rule-based system on the MC160, with no additional transformations, is almost 10\% more accurate than the same system running on the MC500. However, the MC160 registers smaller improvements, or even decreases, in accuracy when more sophisticated components, such as coreference and hypernymy, are added.

This is a consequence of the design and curation process of the MC500 corpus, which required that answers must not be contained directly within the story text, or, if they are, two or more misleading answers must also be included. This decision portrays the MC500 as a textual entailment problem, and when \newcite{mctest} utilised an RTE system in tandem with their lexical baseline, they noted a drastic increase in accuracy on the MC500, and a minor increase on the MC160.

However, in many cases, the MC500 appears to be over-engineered to confuse lexical algorithms, and to encourage the use of entailment processes, by including complex sequences of negation and inference, such as in Figure \ref{fig:exampleMC500hard}. But, from our observations, the MC500 is a more accurate measure of machine comprehension, and will require a system with genuine textual understanding to solve.

\begin{figure}[!th]
\input{./figures/exampleMC500hard.tex}
\caption{\label{fig:exampleMC500hard} A question from the MC500 test set.}
\end{figure}

\section{Future work and conclusion}
\input{./sections/FutureWorkAndConclusion.tex}

\section*{Acknowledgments}
\input{./sections/Acknowledgements.tex}

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{ref}
\end{document}
