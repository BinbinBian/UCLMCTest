% Within the field of NLP, techniques are often evaluated using narrowly defined tasks (such as recognising textual entailment, information extraction and semantic role labelling), that attempt to quantify progress in a specific subdomain of the task of the machine comprehension of text (MCT). This makes direct comparisons between a wide range of NLP systems somewhat prohibitive, and as such could well hinder the transfer of knowledge from the academic use of these techniques with controlled datasets, to the real world application of the research.

% MCTest \cite{mctest} is a task that allows the evaluation of techniques from multiple subdomains using the high-level goal of MCT, enabling more diverse comparisons across the field of NLP to be made and providing an assessment of the field's progress towards achieving real world MCT. 

Machine comprehension of text is a central goal in NLP. The academic community has proposed different challenges to measure progress, stimulating research in several fields: information extraction, semantic parsing and textual entailment. Yet these challenges often evaluate how we perform on individual techniques rather than how far we are from machine comprehension of text. A recent contribution proposes MCTest \cite{mctest}, a new challenge that aims at evaluating machine comprehension through an open-domain question answering task, requiring the common sense reasoning typical of a 7 year-old child. 

Our initial approach to tackling MCTest incorporates a simple bag of words algorithm as a baseline, which is then enhanced by adding pre-processing features such as co-reference resolution and finding hypernymy relations in the text. We then ensemble multiple approaches using various pre-processing features and in addition, use different variations of the original baseline algorithm, along with the use of a sentence selection algorithm to further improve the accuracy of the baseline. A linear SVM is then trained using the aforementioned features to classify a question-answer pair according to whether it is correct.

In order to analyse the MCTest dataset, we then construct a second, rule-based system that categorises the question-answer pairs and subsequently applies transformations to the story text. Words within the text are then weighted depending on the question category, and then passed to a scoring function based upon the Sliding Window with Distance algorithm seen in \cite{mctest}. This system allows us to gain a quantitative understanding of the dataset, by allowing us to see the individual effects of each component of the system on a question by question basis. Using the data that this generates, we evaluate the relative performances that various features have on the dataset, then also analyse the differences between the MC160 and MC500 collections as well as the type of questions that appear in each collection.